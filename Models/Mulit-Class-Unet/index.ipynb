{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84dd9fc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wrapt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_38836/2026023513.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# import skimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# from skimage.transform import resize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# Bring in subpackages.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m# from tensorflow.python import keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAUTOTUNE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mservice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdense_to_ragged_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdense_to_sparse_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfrom_dataset_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregister_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompression_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_options\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoShardPolicy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_options\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExternalStatePolicy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgen_experimental_dataset_ops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mged_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mwrapt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wrapt'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os \n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import cv2\n",
    "# import skimage\n",
    "# from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from keras.layers import Activation, MaxPool2D, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db586727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(physical_devices)\n",
    "# if physical_devices:\n",
    "#     tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7862fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(r\"../../Datasets/Dataset_A_resized_256.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bbd2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MR Image shape:   {}\".format(df[\"Tumor Mask\"][0].shape))\n",
    "print(\"Tumor Mask shape: {}\".format(df[\"Tumor Mask\"][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0619f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dict = {\"Not Tumor\": 0, \"Meningioma\": 1, \"Glioma\": 2, \"Pituitary\": 3}\n",
    "for index, row in df.iterrows():\n",
    "    row[\"Tumor Mask\"] = row[\"Tumor Mask\"] * classes_dict[row[\"Labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4acb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    plt.imsave('DR.png', img, cmap='gray')\n",
    "    return cv2.imread('DR.png')[:, :, 0]\n",
    "\n",
    "df.Images = df.Images.apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0bc48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "randInt = np.random.randint(0,len(df))\n",
    "\n",
    "# create figure\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "  \n",
    "# setting values to rows and column variables\n",
    "rows = 1\n",
    "columns = 2\n",
    "\n",
    "img  = df.Images[randInt]\n",
    "mask = df[\"Tumor Mask\"][randInt]\n",
    "tumorBorder = df[\"Tumor Border\"][randInt]\n",
    "\n",
    "\n",
    "# Display Original Image\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title('Original Image ({})'.format(df.Labels[randInt]))\n",
    "plt.axis('off')\n",
    "  \n",
    "# Display Tumor Mask\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.title('Tumor Mask ({})'.format(np.unique(mask)))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting Images form masks\n",
    "img_size = 256\n",
    "\n",
    "Y = df[\"Tumor Mask\"].tolist()\n",
    "X = df['Images'].tolist()\n",
    "\n",
    "## Adding extra Color channel to the grayscale images\n",
    "Y = np.array(Y).reshape(-1, img_size, img_size)\n",
    "Y = Y.astype(float)\n",
    "\n",
    "X = np.array(X).reshape(-1, img_size, img_size)\n",
    "\n",
    "print(f\"shape of images: {X.shape}\\nshape of labels: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d45b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train_masks = Y\n",
    "labelencoder = LabelEncoder()\n",
    "n, h, w = train_masks.shape\n",
    "train_masks_reshaped = train_masks.reshape(-1,1)\n",
    "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped.ravel())\n",
    "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
    "\n",
    "np.unique(train_masks_encoded_original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import normalize\n",
    "\n",
    "train_images = np.expand_dims(X, axis=3)\n",
    "train_images = normalize(train_images, axis=1)\n",
    "\n",
    "train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e80731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "n_classes = len(np.unique(train_masks_input))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_images, train_masks_input, test_size=0.25, random_state=42)\n",
    "print(\"Class values in the dataset are ... \", np.unique(y_train))  # 0 is the background/few unlabeled \n",
    "\n",
    "\n",
    "train_masks_cat = to_categorical(y_train, num_classes=n_classes)\n",
    "y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))\n",
    "\n",
    "\n",
    "\n",
    "test_masks_cat = to_categorical(y_test, num_classes=n_classes)\n",
    "y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2fc0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape,\"| y_train shape:\", y_train_cat.shape)\n",
    "print(\"X_test shape :\",X_test.shape,\"| y_test shape : \", y_test_cat.shape)\n",
    "\n",
    "print(\"\\nTotal Data: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ada49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y_test_cat[0, :, :, 0], 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d29f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y_test_cat[0, :, :, 1], 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e8ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y_test_cat[0, :, :, 2], 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd24560",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y_test_cat[0, :, :, 3], 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf27a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test[0, :, :, :], 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3380d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0, :, :, :].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcedf94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da3988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02fb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                 classes = np.unique(train_masks_reshaped_encoded),\n",
    "                                                 y = train_masks_reshaped_encoded)\n",
    "print(\"Class weights are...:\", class_weights)\n",
    "\n",
    "\n",
    "IMG_HEIGHT = X_train.shape[1]\n",
    "IMG_WIDTH  = X_train.shape[2]\n",
    "IMG_CHANNELS = X_train.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093a66ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Unet by dividing encoder and decoder into blocks\n",
    "def conv_block(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)   #Not in the original network. \n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)  #Not in the original network\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "#Encoder block: Conv block followed by maxpooling\n",
    "\n",
    "\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = MaxPool2D((2, 2))(x)\n",
    "    return x, p   \n",
    "\n",
    "#Decoder block\n",
    "#skip features gets input from encoder for concatenation\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "#Build Unet using the blocks\n",
    "def build_unet(n_classes, input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    b1 = conv_block(p4, 1024) #Bridge\n",
    "\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(d4)  #Binary (can be multiclass)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa40f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "exp_num = []\n",
    "if not os.path.exists(\"./experiments\"):\n",
    "    os.mkdir(\"./experiments\")\n",
    "folders = os.listdir(\"./experiments\")\n",
    "for folder in folders:\n",
    "    exp_num.append(int(folder.split(\"#\")[1]))\n",
    "\n",
    "\n",
    "if len(exp_num) == 0:\n",
    "    exp_path = r\"./experiments/exp#1\"\n",
    "else:\n",
    "    exp_path = r\"./experiments/exp#\"+str(max(exp_num)+1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.mkdir(exp_path)\n",
    "os.mkdir(os.path.join(exp_path, \"results\"))\n",
    "os.mkdir(os.path.join(exp_path, \"weights\"))\n",
    "\n",
    "results = os.path.join(exp_path, \"results\")\n",
    "weights = os.path.join(exp_path, \"weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737080fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "\n",
    "model = build_unet(n_classes, input_shape)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4badb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# model_checkpoint_callback = ModelCheckpoint(\n",
    "#     filepath= os.path.join(weights, \"best.h5\"),\n",
    "#     monitor='val_accuracy', \n",
    "#     verbose=1, \n",
    "#     save_best_only=False, \n",
    "#     mode='max')\n",
    "    \n",
    "model.fit(X_train, \n",
    "        y_train_cat, \n",
    "        batch_size = 1, \n",
    "        verbose=1, \n",
    "        epochs=60, \n",
    "        validation_data=(X_test, y_test_cat), \n",
    "#         class_weight=class_weights,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fced661",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model.evaluate(X_test, y_test_cat)[0]\n",
    "name = str(round(acc, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save The Trained model along with it training history   \n",
    "metrics.to_pickle(os.path.join(results, name+'.pkl'))\n",
    "m1.save(os.path.join(weights, name + '.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce2e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IOU\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_argmax=np.argmax(y_pred, axis=3)\n",
    "\n",
    "##################################################\n",
    "\n",
    "#Using built in keras function\n",
    "from keras.metrics import MeanIoU\n",
    "n_classes = 4\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(y_test[:,:,:,0], y_pred_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "\n",
    "#To calculate I0U for each class...\n",
    "values = np.array(IOU_keras.get_weights()).reshape(n_classes, n_classes)\n",
    "print(values)\n",
    "class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3] + values[1,0]+ values[2,0]+ values[3,0])\n",
    "class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3] + values[0,1]+ values[2,1]+ values[3,1])\n",
    "class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[2,3] + values[0,2]+ values[1,2]+ values[3,2])\n",
    "class4_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2] + values[0,3]+ values[1,3]+ values[2,3])\n",
    "\n",
    "print(\"IoU for class1 is: \", class1_IoU)\n",
    "print(\"IoU for class2 is: \", class2_IoU)\n",
    "print(\"IoU for class3 is: \", class3_IoU)\n",
    "print(\"IoU for class4 is: \", class4_IoU)\n",
    "\n",
    "plt.imshow(train_images[0, :,:,0], cmap='gray')\n",
    "plt.imshow(train_masks[0], cmap='gray')\n",
    "#######################################################################\n",
    "#Predict on a few images\n",
    "#model = get_model()\n",
    "#model.load_weights('???.hdf5')  \n",
    "import random\n",
    "test_img_number = random.randint(0, len(X_test))\n",
    "test_img = X_test[test_img_number]\n",
    "ground_truth=y_test[test_img_number]\n",
    "test_img_norm=test_img[:,:,0][:,:,None]\n",
    "test_img_input=np.expand_dims(test_img_norm, 0)\n",
    "prediction = (model.predict(test_img_input))\n",
    "predicted_img=np.argmax(prediction, axis=3)[0,:,:]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(test_img[:,:,0], cmap='gray')\n",
    "plt.subplot(232)\n",
    "plt.title('Testing Label')\n",
    "plt.imshow(ground_truth[:,:,0], cmap='jet')\n",
    "plt.subplot(233)\n",
    "plt.title('Prediction on test image')\n",
    "plt.imshow(predicted_img, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1745cf00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93122ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from simple_multi_unet_model import multi_unet_model #Uses softmax \n",
    "\n",
    "# from keras.utils import normalize\n",
    "# import os\n",
    "# import glob\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# #Resizing images, if needed\n",
    "# SIZE_X = 256 \n",
    "# SIZE_Y = 256\n",
    "# n_classes=4 #Number of classes for segmentation\n",
    "\n",
    "# #Capture training image info as a list\n",
    "# train_images = []\n",
    "\n",
    "# for directory_path in glob.glob(\"128_patches/images/\"):\n",
    "#     for img_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
    "#         img = cv2.imread(img_path, 0)       \n",
    "#         #img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
    "#         train_images.append(img)\n",
    "       \n",
    "# #Convert list to array for machine learning processing        \n",
    "# train_images = np.array(train_images)\n",
    "\n",
    "# #Capture mask/label info as a list\n",
    "# train_masks = [] \n",
    "# for directory_path in glob.glob(\"128_patches/masks/\"):\n",
    "#     for mask_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
    "#         mask = cv2.imread(mask_path, 0)       \n",
    "#         #mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)  #Otherwise ground truth changes due to interpolation\n",
    "#         train_masks.append(mask)\n",
    "        \n",
    "# #Convert list to array for machine learning processing          \n",
    "# train_masks = np.array(train_masks)\n",
    "\n",
    "# ###############################################\n",
    "# #Encode labels... but multi dim array so need to flatten, encode and reshape\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# labelencoder = LabelEncoder()\n",
    "# n, h, w = train_masks.shape\n",
    "# train_masks_reshaped = train_masks.reshape(-1,1)\n",
    "# train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
    "# train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
    "\n",
    "# np.unique(train_masks_encoded_original_shape)\n",
    "\n",
    "# #################################################\n",
    "# train_images = np.expand_dims(train_images, axis=3)\n",
    "# train_images = normalize(train_images, axis=1)\n",
    "\n",
    "# train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)\n",
    "\n",
    "# #Create a subset of data for quick testing\n",
    "# #Picking 10% for testing and remaining for training\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X1, X_test, y1, y_test = train_test_split(train_images, train_masks_input, test_size = 0.10, random_state = 0)\n",
    "\n",
    "# #Further split training data t a smaller subset for quick testing of models\n",
    "# X_train, X_do_not_use, y_train, y_do_not_use = train_test_split(X1, y1, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# print(\"Class values in the dataset are ... \", np.unique(y_train))  # 0 is the background/few unlabeled \n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "# train_masks_cat = to_categorical(y_train, num_classes=n_classes)\n",
    "# y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))\n",
    "\n",
    "\n",
    "\n",
    "# test_masks_cat = to_categorical(y_test, num_classes=n_classes)\n",
    "# y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))\n",
    "\n",
    "\n",
    "\n",
    "# ###############################################################\n",
    "# from sklearn.utils import class_weight\n",
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                  np.unique(train_masks_reshaped_encoded),\n",
    "#                                                  train_masks_reshaped_encoded)\n",
    "# print(\"Class weights are...:\", class_weights)\n",
    "\n",
    "\n",
    "# IMG_HEIGHT = X_train.shape[1]\n",
    "# IMG_WIDTH  = X_train.shape[2]\n",
    "# IMG_CHANNELS = X_train.shape[3]\n",
    "\n",
    "# def get_model():\n",
    "#     return multi_unet_model(n_classes=n_classes, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_CHANNELS=IMG_CHANNELS)\n",
    "\n",
    "# model = get_model()\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# #If starting with pre-trained weights. \n",
    "# #model.load_weights('???.hdf5')\n",
    "\n",
    "# history = model.fit(X_train, y_train_cat, \n",
    "#                     batch_size = 16, \n",
    "#                     verbose=1, \n",
    "#                     epochs=50, \n",
    "#                     validation_data=(X_test, y_test_cat), \n",
    "#                     #class_weight=class_weights,\n",
    "#                     shuffle=False)\n",
    "                    \n",
    "\n",
    "\n",
    "# model.save('test.hdf5')\n",
    "# #model.save('sandstone_50_epochs_catXentropy_acc_with_weights.hdf5')\n",
    "# ############################################################\n",
    "# #Evaluate the model\n",
    "# \t# evaluate model\n",
    "# _, acc = model.evaluate(X_test, y_test_cat)\n",
    "# print(\"Accuracy is = \", (acc * 100.0), \"%\")\n",
    "\n",
    "\n",
    "# ###\n",
    "# #plot the training and validation accuracy and loss at each epoch\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# epochs = range(1, len(loss) + 1)\n",
    "# plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# acc = history.history['acc']\n",
    "# val_acc = history.history['val_acc']\n",
    "\n",
    "# plt.plot(epochs, acc, 'y', label='Training Accuracy')\n",
    "# plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "# plt.title('Training and validation Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# ##################################\n",
    "# #model = get_model()\n",
    "# model.load_weights('sandstone_50_epochs_catXentropy_acc.hdf5')  \n",
    "# #model.load_weights('sandstone_50_epochs_catXentropy_acc_with_weights.hdf5')  \n",
    "\n",
    "# #IOU\n",
    "# y_pred=model.predict(X_test)\n",
    "# y_pred_argmax=np.argmax(y_pred, axis=3)\n",
    "\n",
    "# ##################################################\n",
    "\n",
    "# #Using built in keras function\n",
    "# from keras.metrics import MeanIoU\n",
    "# n_classes = 4\n",
    "# IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "# IOU_keras.update_state(y_test[:,:,:,0], y_pred_argmax)\n",
    "# print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "\n",
    "# #To calculate I0U for each class...\n",
    "# values = np.array(IOU_keras.get_weights()).reshape(n_classes, n_classes)\n",
    "# print(values)\n",
    "# class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3] + values[1,0]+ values[2,0]+ values[3,0])\n",
    "# class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3] + values[0,1]+ values[2,1]+ values[3,1])\n",
    "# class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[2,3] + values[0,2]+ values[1,2]+ values[3,2])\n",
    "# class4_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2] + values[0,3]+ values[1,3]+ values[2,3])\n",
    "\n",
    "# print(\"IoU for class1 is: \", class1_IoU)\n",
    "# print(\"IoU for class2 is: \", class2_IoU)\n",
    "# print(\"IoU for class3 is: \", class3_IoU)\n",
    "# print(\"IoU for class4 is: \", class4_IoU)\n",
    "\n",
    "# plt.imshow(train_images[0, :,:,0], cmap='gray')\n",
    "# plt.imshow(train_masks[0], cmap='gray')\n",
    "# #######################################################################\n",
    "# #Predict on a few images\n",
    "# #model = get_model()\n",
    "# #model.load_weights('???.hdf5')  \n",
    "# import random\n",
    "# test_img_number = random.randint(0, len(X_test))\n",
    "# test_img = X_test[test_img_number]\n",
    "# ground_truth=y_test[test_img_number]\n",
    "# test_img_norm=test_img[:,:,0][:,:,None]\n",
    "# test_img_input=np.expand_dims(test_img_norm, 0)\n",
    "# prediction = (model.predict(test_img_input))\n",
    "# predicted_img=np.argmax(prediction, axis=3)[0,:,:]\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.subplot(231)\n",
    "# plt.title('Testing Image')\n",
    "# plt.imshow(test_img[:,:,0], cmap='gray')\n",
    "# plt.subplot(232)\n",
    "# plt.title('Testing Label')\n",
    "# plt.imshow(ground_truth[:,:,0], cmap='jet')\n",
    "# plt.subplot(233)\n",
    "# plt.title('Prediction on test image')\n",
    "# plt.imshow(predicted_img, cmap='jet')\n",
    "# plt.show()\n",
    "\n",
    "# #####################################################################\n",
    "\n",
    "# #Predict on large image\n",
    "\n",
    "# #Apply a trained model on large image\n",
    "\n",
    "# from patchify import patchify, unpatchify\n",
    "\n",
    "# large_image = cv2.imread('large_images/large_image.tif', 0)\n",
    "# #This will split the image into small images of shape [3,3]\n",
    "# patches = patchify(large_image, (128, 128), step=128)  #Step=256 for 256 patches means no overlap\n",
    "\n",
    "# predicted_patches = []\n",
    "# for i in range(patches.shape[0]):\n",
    "#     for j in range(patches.shape[1]):\n",
    "#         print(i,j)\n",
    "        \n",
    "#         single_patch = patches[i,j,:,:]       \n",
    "#         single_patch_norm = np.expand_dims(normalize(np.array(single_patch), axis=1),2)\n",
    "#         single_patch_input=np.expand_dims(single_patch_norm, 0)\n",
    "#         single_patch_prediction = (model.predict(single_patch_input))\n",
    "#         single_patch_predicted_img=np.argmax(single_patch_prediction, axis=3)[0,:,:]\n",
    "\n",
    "#         predicted_patches.append(single_patch_predicted_img)\n",
    "\n",
    "# predicted_patches = np.array(predicted_patches)\n",
    "\n",
    "# predicted_patches_reshaped = np.reshape(predicted_patches, (patches.shape[0], patches.shape[1], 128,128) )\n",
    "\n",
    "# reconstructed_image = unpatchify(predicted_patches_reshaped, large_image.shape)\n",
    "# plt.imshow(reconstructed_image, cmap='gray')\n",
    "# #plt.imsave('data/results/segm.jpg', reconstructed_image, cmap='gray')\n",
    "\n",
    "# plt.hist(reconstructed_image.flatten())  #Threshold everything above 0\n",
    "\n",
    "# # final_prediction = (reconstructed_image > 0.01).astype(np.uint8)\n",
    "# # plt.imshow(final_prediction)\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.subplot(221)\n",
    "# plt.title('Large Image')\n",
    "# plt.imshow(large_image, cmap='gray')\n",
    "# plt.subplot(222)\n",
    "# plt.title('Prediction of large Image')\n",
    "# plt.imshow(reconstructed_image, cmap='jet')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
